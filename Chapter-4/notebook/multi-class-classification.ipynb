{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0da811a",
   "metadata": {},
   "source": [
    "# 4章 多クラス分類の評価指標"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba038b94",
   "metadata": {},
   "source": [
    "## 4.3 混合行列(Confusion Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d147efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import pandas\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 文字列や数値で表されたラベルを0~(ラベル種類数-1)に変換する関数\n",
    "def label_encoding(df):\n",
    "    object_type_column_list = [c for c in df.columns if df[c].dtypes=='object']\n",
    "    for object_type_column in object_type_column_list:\n",
    "        label_encoder = LabelEncoder()\n",
    "        df[object_type_column] = label_encoder.fit_transform(df[object_type_column])\n",
    "    return df\n",
    "\n",
    "# 学習用データの読み込み\n",
    "train_df = pandas.read_csv(\"../data/customer-segmentation/train.csv\")\n",
    "\n",
    "# 目的変数を抽出\n",
    "y = train_df.pop('Segmentation').values\n",
    "# 本当はあまり良くないがNanを全て-1で埋める\n",
    "train_df = label_encoding(train_df).fillna(-1)\n",
    "# 顧客のユニークなIDを含むとノイズになるのでカラムを削除する\n",
    "train_df = train_df.drop(\"ID\", axis=1)\n",
    "\n",
    "# 学習データと検証用データに分ける\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# モデルは決定木を利用する\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 検証用データで予測\n",
    "y_val_hat = clf.predict(X_val)\n",
    "\n",
    "# 混同行列を作成\n",
    "conf_matrix = confusion_matrix(y_val, y_val_hat)\n",
    "\n",
    "# 分類に用いられるクラス名を定義する\n",
    "labels = [\"A\", \"B\", \"C\", \"D\"]\n",
    "df_cm = pandas.DataFrame(conf_matrix, index=labels, columns=labels)\n",
    "# 混同行列をヒートマップで描画する\n",
    "sns.heatmap(df_cm,  fmt=\"d\", annot=True, cmap=\"binary\", annot_kws={\"fontsize\": 20})\n",
    "# グラフにタイトルをつける\n",
    "plt.title('Confusion Matrix')\n",
    "# x軸、y軸の名前をグラフに記載する\n",
    "plt.xlabel(\"Predicted class\")\n",
    "plt.ylabel(\"True class\")\n",
    "# グラフを表示する\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f630cccf",
   "metadata": {},
   "source": [
    "## 正解率 (Multi class accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfef696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# ダミーデータを作る\n",
    "X, y = make_classification(n_samples = 10000, random_state=42, n_classes=3, n_clusters_per_class=1, weights = [0.899, 0.1, 0.001])\n",
    "X_train, X_test, y_train, y_val_dummy = train_test_split(\n",
    "        X, y, random_state=42)\n",
    "\n",
    "# モデルは決定木を利用する\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 分類に用いられるクラス名を定義する\n",
    "labels = [0, 1, 2]\n",
    "# 検証用データで予測\n",
    "y_val_dummy_hat = clf.predict(X_test)\n",
    "# 混同行列を作成\n",
    "conf_matrix = confusion_matrix(y_val_dummy, y_val_dummy_hat, labels=labels)\n",
    "# 混同行列をヒートマップで描画する\n",
    "sns.heatmap(conf_matrix, fmt=\"d\", annot=True, cmap=\"binary\", annot_kws={\"fontsize\": 20})\n",
    "# グラフにタイトルをつける\n",
    "plt.title('Confusion Matrix')\n",
    "# x軸、y軸の名前をグラフに記載する\n",
    "plt.xlabel(\"Predicted class\")\n",
    "plt.ylabel(\"True class\")\n",
    "# グラフを表示する\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6262aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 正解率による評価\n",
    "print(\"不均衡データでの正解率:\", round(accuracy_score(y_val_dummy, y_val_dummy_hat), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4e169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 正解率による評価\n",
    "print(\"customer-segmentationデータセットでの正解率:\", round(accuracy_score(y_val, y_val_hat), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c83e21a",
   "metadata": {},
   "source": [
    "## Micro precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8436d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# averageという引数に'micro'という文字列を渡すと\n",
    "# Micro Precisionを計算する\n",
    "print(\"不均衡データでのMicro Precision:\", round(precision_score(y_val_dummy, y_val_dummy_hat, average='micro'), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a1b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Micro precisionによる評価(正解率と同じ値になる)\n",
    "print(\"customer-segmentationデータセットでのMicro Precision:\", round(precision_score(y_val, y_val_hat, average=\"micro\"), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1d0e82",
   "metadata": {},
   "source": [
    "## Macro precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4977f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# ダミーデータを作る\n",
    "X, y = make_classification(n_samples = 10000, random_state=42, n_classes=3, n_clusters_per_class=1, weights = [0.899, 0.1, 0.001])\n",
    "# 学習データと検証用データに分ける\n",
    "X_train, X_test, y_train, y_val_dummy = train_test_split(\n",
    "        X, y, random_state=42)\n",
    "\n",
    "# モデルは決定木を利用する\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 分類に用いられるクラス名を定義する\n",
    "labels = [0, 1, 2]\n",
    "# 検証用データで予測\n",
    "y_val_dummy_hat = clf.predict(X_test)\n",
    "\n",
    "# averageという引数に'macro'という文字列を渡すと\n",
    "# Macro Precisionを計算する\n",
    "print(\"不均衡データでのMacro Precision:\", round(precision_score(y_val_dummy, y_val_dummy_hat, average='macro'), 3))\n",
    "\n",
    "# averageという引数に'micro'という文字列を渡すと\n",
    "# Micro Precisionを計算する\n",
    "print(\"不均衡データでのMicro Precision:\", round(precision_score(y_val_dummy, y_val_dummy_hat, average='micro'), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0050bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用データの読み込み\n",
    "train_df = pandas.read_csv(\"../data/customer-segmentation/train.csv\")\n",
    "\n",
    "# 目的変数を抽出\n",
    "y = train_df.pop('Segmentation').values\n",
    "# 本当はあまり良くないがNanを全て-1で埋める\n",
    "train_df = label_encoding(train_df).fillna(-1)\n",
    "# 顧客のユニークなIDを含むとノイズになるのでカラムを削除する\n",
    "train_df = train_df.drop(\"ID\", axis=1)\n",
    "\n",
    "# 学習データと検証用データに分ける\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# モデルは決定木を利用する\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 検証用データで予測\n",
    "y_val_hat = clf.predict(X_val)\n",
    "\n",
    "# averageという引数に'macro'という文字列を渡すと\n",
    "# Macro precisionを計算する\n",
    "print(\"customer-segmentationデータセットでのMacro Precision\", round(precision_score(y_val, y_val_hat, average=\"macro\"), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd9f2c5",
   "metadata": {},
   "source": [
    "## Weighted precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868524bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ダミーデータを作る\n",
    "X, y = make_classification(n_samples = 10000, random_state=42, n_classes=3, n_clusters_per_class=1, weights = [0.899, 0.1, 0.001])\n",
    "X_train, X_test, y_train, y_val_dummy = train_test_split(\n",
    "        X, y, random_state=42)\n",
    "\n",
    "# モデルは決定木を利用する\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 分類に用いられるクラス名を定義する\n",
    "labels = [0, 1, 2]\n",
    "# 検証用データで予測\n",
    "y_val_dummy_hat = clf.predict(X_test)\n",
    "\n",
    "# averageという引数に'weighted'という文字列を渡すと\n",
    "# Weighted Precisionを計算する\n",
    "print(\"不均衡データでのWeighted Precision:\", round(precision_score(y_val_dummy, y_val_dummy_hat, average='weighted'), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39218624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用データの読み込み\n",
    "train_df = pandas.read_csv(\"../data/customer-segmentation/train.csv\")\n",
    "\n",
    "# 目的変数を抽出\n",
    "y = train_df.pop('Segmentation').values\n",
    "# 本当はあまり良くないがNanを全て-1で埋める\n",
    "train_df = label_encoding(train_df).fillna(-1)\n",
    "# 顧客のユニークなIDを含むとノイズになるのでカラムを削除する\n",
    "train_df = train_df.drop(\"ID\", axis=1)\n",
    "\n",
    "# 学習データと検証用データに分ける\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# モデルは決定木を利用する\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 検証用データで予測\n",
    "y_val_hat = clf.predict(X_val)\n",
    "\n",
    "# averageという引数に'weighted'という文字列を渡すと\n",
    "# Weighted precisionを計算する\n",
    "print(\"customer-segmentationデータセットでのWeighted Precision:\", round(precision_score(y_val, y_val_hat, average='weighted'), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01d8b8b",
   "metadata": {},
   "source": [
    "## Micro recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18ee794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# ダミーデータを作る\n",
    "X, y = make_classification(n_samples = 10000, random_state=42, n_classes=3, n_clusters_per_class=1, weights = [0.899, 0.1, 0.001])\n",
    "# 学習データと検証用データに分ける\n",
    "X_train, X_test, y_train, y_val_dummy = train_test_split(\n",
    "        X, y, random_state=42)\n",
    "\n",
    "# モデルは決定木を利用する\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 分類に用いられるクラス名を定義する\n",
    "labels = [0, 1, 2]\n",
    "# 検証用データで予測\n",
    "y_val_dummy_hat = clf.predict(X_test)\n",
    "\n",
    "# averageという引数に'micro'という文字列を渡すと\n",
    "# Micro Recallを計算する\n",
    "print(\"不均衡データセットでのMicro Recall:\", round(recall_score(y_val_dummy, y_val_dummy_hat, average='micro'), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be70a0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用データの読み込み\n",
    "train_df = pandas.read_csv(\"../data/customer-segmentation/train.csv\")\n",
    "\n",
    "# 目的変数を抽出\n",
    "y = train_df.pop('Segmentation').values\n",
    "# 本当はあまり良くないがNanを全て-1で埋める\n",
    "train_df = label_encoding(train_df).fillna(-1)\n",
    "# 顧客のユニークなIDを含むとノイズになるのでカラムを削除する\n",
    "train_df = train_df.drop(\"ID\", axis=1)\n",
    "\n",
    "# 学習データと検証用データに分ける\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# モデルは決定木を利用する\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 検証用データで予測\n",
    "y_val_hat = clf.predict(X_val)\n",
    "\n",
    "# averageという引数に'micro'という文字列を渡すと\n",
    "# Micro recallを計算する\n",
    "recall_score(y_val, y_val_hat, average='micro')\n",
    "print(\"customer-segmentationデータセットでのMicro Recall:\", round(recall_score(y_val, y_val_hat, average='micro'), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40deb2a",
   "metadata": {},
   "source": [
    "## Macro recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea34c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# ダミーデータを作る\n",
    "X, y = make_classification(n_samples = 10000, random_state=42, n_classes=3, n_clusters_per_class=1, weights = [0.899, 0.1, 0.001])\n",
    "# 学習データと検証用データに分ける\n",
    "X_train, X_test, y_train, y_val_dummy = train_test_split(\n",
    "        X, y, random_state=42)\n",
    "\n",
    "# モデルは決定木を利用する\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 分類に用いられるクラス名を定義する\n",
    "labels = [0, 1, 2]\n",
    "# 検証用データで予測\n",
    "y_val_dummy_hat = clf.predict(X_test)\n",
    "\n",
    "# averageという引数に'macro'という文字列を渡すと\n",
    "# Macro recallを計算する\n",
    "print(\"不均衡データセットでのMacro Recall:\", round(recall_score(y_val_dummy, y_val_dummy_hat, average='macro'), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322b0dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用データの読み込み\n",
    "train_df = pandas.read_csv(\"../data/customer-segmentation/train.csv\")\n",
    "\n",
    "# 目的変数を抽出\n",
    "y = train_df.pop('Segmentation').values\n",
    "# 本当はあまり良くないがNanを全て-1で埋める\n",
    "train_df = label_encoding(train_df).fillna(-1)\n",
    "# 顧客のユニークなIDを含むとノイズになるのでカラムを削除する\n",
    "train_df = train_df.drop(\"ID\", axis=1)\n",
    "\n",
    "# 学習データと検証用データに分ける\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# モデルは決定木を利用する\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 検証用データで予測\n",
    "y_val_hat = clf.predict(X_val)\n",
    "\n",
    "# averageという引数に'macro'という文字列を渡すと\n",
    "# Macro recallを計算する\n",
    "print(\"customer-segmentationデータセットでのMacro Recall:\", round(recall_score(y_val, y_val_hat, average='macro'), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b711a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# averageという引数にNoneを渡すとクラスごとの\n",
    "# Recallを計算する\n",
    "print(recall_score(y_val, y_val_hat, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b335ae",
   "metadata": {},
   "source": [
    "## Weighted Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739c2e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# ダミーデータを作る\n",
    "X, y = make_classification(n_samples = 10000, random_state=42, n_classes=3, n_clusters_per_class=1, weights = [0.899, 0.1, 0.001])\n",
    "# 学習データと検証用データに分ける\n",
    "X_train, X_test, y_train, y_val_dummy = train_test_split(\n",
    "        X, y, random_state=42)\n",
    "\n",
    "# モデルは決定木を利用する\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 分類に用いられるクラス名を定義する\n",
    "labels = [0, 1, 2]\n",
    "# 検証用データで予測\n",
    "y_val_dummy_hat = clf.predict(X_test)\n",
    "\n",
    "# averageという引数に'weighted'という文字列を渡すと\n",
    "# Weighted recallを計算する\n",
    "print(\"不均衡データでのWeighted Recall:\", round(recall_score(y_val_dummy, y_val_dummy_hat, average='weighted'), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用データの読み込み\n",
    "train_df = pandas.read_csv(\"../data/customer-segmentation/train.csv\")\n",
    "\n",
    "# 目的変数を抽出\n",
    "y = train_df.pop('Segmentation').values\n",
    "# 本当はあまり良くないがNanを全て-1で埋める\n",
    "train_df = label_encoding(train_df).fillna(-1)\n",
    "# 顧客のユニークなIDを含むとノイズになるのでカラムを削除する\n",
    "train_df = train_df.drop(\"ID\", axis=1)\n",
    "\n",
    "# 学習データと検証用データに分ける\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# モデルは決定木を利用する\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 検証用データで予測\n",
    "y_val_hat = clf.predict(X_val)\n",
    "\n",
    "# averageという引数に'weighted'という文字列を渡すと\n",
    "# Weighted recallを計算する\n",
    "print(\"customer-segmentationデータセットでのWeighted Recall:\", round(recall_score(y_val, y_val_hat, average='weighted'), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c36bd3",
   "metadata": {},
   "source": [
    "## Micro F1スコア"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1665da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ダミーデータを作る\n",
    "X, y = make_classification(n_samples = 10000, random_state=42, n_classes=3, n_clusters_per_class=1, weights = [0.899, 0.1, 0.001])\n",
    "X_train, X_test, y_train, y_val_dummy = train_test_split(\n",
    "        X, y, random_state=42)\n",
    "\n",
    "# モデルは決定木を利用する\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 分類に用いられるクラス名を定義する\n",
    "labels = [0, 1, 2]\n",
    "# 検証用データで予測\n",
    "y_val_dummy_hat = clf.predict(X_test)\n",
    "\n",
    "# averageという引数に'micro'という文字列を渡すと\n",
    "# Micro F1スコアを計算する\n",
    "print(\"不均衡データでのMicro F1スコア:\", round(f1_score(y_val_dummy, y_val_dummy_hat, average='micro'), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef0e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用データの読み込み\n",
    "train_df = pandas.read_csv(\"../data/customer-segmentation/train.csv\")\n",
    "\n",
    "# 目的変数を抽出\n",
    "y = train_df.pop('Segmentation').values\n",
    "# 本当はあまり良くないがNanを全て-1で埋める\n",
    "train_df = label_encoding(train_df).fillna(-1)\n",
    "# 顧客のユニークなIDを含むとノイズになるのでカラムを削除する\n",
    "train_df = train_df.drop(\"ID\", axis=1)\n",
    "\n",
    "# 学習データと検証用データに分ける\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# モデルは決定木を利用する\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 検証用データで予測\n",
    "y_val_hat = clf.predict(X_val)\n",
    "\n",
    "# averageという引数に'micro'という文字列を渡すと\n",
    "# Micro F1スコアを計算する\n",
    "print(\"customer-segmentationデータセットでのMicro F1スコア:\", round(f1_score(y_val, y_val_hat, average='micro'), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1228e28e",
   "metadata": {},
   "source": [
    "## Macro F1スコア"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc14138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ダミーデータを作る\n",
    "X, y = make_classification(n_samples = 10000, random_state=42, n_classes=3, n_clusters_per_class=1, weights = [0.899, 0.1, 0.001])\n",
    "# 学習データと検証用データに分ける\n",
    "X_train, X_test, y_train, y_val_dummy = train_test_split(\n",
    "        X, y, random_state=42)\n",
    "\n",
    "# モデルは決定木を利用する\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 分類に用いられるクラス名を定義する\n",
    "labels = [0, 1, 2]\n",
    "# 検証用データで予測\n",
    "y_val_dummy_hat = clf.predict(X_test)\n",
    "\n",
    "# averageという引数に'macro'という文字列を渡すと\n",
    "# Macro F1スコアを計算する\n",
    "print(\"不均衡データでのMacro F1スコア:\", round(f1_score(y_val_dummy, y_val_dummy_hat, average='macro'), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b1edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用データの読み込み\n",
    "train_df = pandas.read_csv(\"../data/customer-segmentation/train.csv\")\n",
    "\n",
    "# 目的変数を抽出\n",
    "y = train_df.pop('Segmentation').values\n",
    "# 本当はあまり良くないがNanを全て-1で埋める\n",
    "train_df = label_encoding(train_df).fillna(-1)\n",
    "# 顧客のユニークなIDを含むとノイズになるのでカラムを削除する\n",
    "train_df = train_df.drop(\"ID\", axis=1)\n",
    "\n",
    "# 学習データと検証用データに分ける\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# モデルは決定木を利用する\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 検証用データで予測\n",
    "y_val_hat = clf.predict(X_val)\n",
    "\n",
    "# averageという引数に'macro'という文字列を渡すと\n",
    "# Macro F1スコアを計算する\n",
    "print(\"customer-segmentationデータセットでのMacro F1スコア:\", round(f1_score(y_val, y_val_hat, average='macro'), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abb2c5f",
   "metadata": {},
   "source": [
    "## Weighted F1スコア"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce1104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ダミーデータを作る\n",
    "X, y = make_classification(n_samples = 10000, random_state=42, n_classes=3, n_clusters_per_class=1, weights = [0.899, 0.1, 0.001])\n",
    "# 学習データと検証用データに分ける\n",
    "X_train, X_test, y_train, y_val_dummy = train_test_split(\n",
    "        X, y, random_state=42)\n",
    "\n",
    "# モデルは決定木を利用する\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 分類に用いられるクラス名を定義する\n",
    "labels = [0, 1, 2]\n",
    "# 検証用データで予測\n",
    "y_val_dummy_hat = clf.predict(X_test)\n",
    "\n",
    "# averageという引数に'weighted'という文字列を渡すと\n",
    "# Weighted f1_scoreを計算する\n",
    "print(\"不均衡データでのWeighted F1スコア:\", round(f1_score(y_val_dummy, y_val_dummy_hat, average='weighted'), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda9898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用データの読み込み\n",
    "train_df = pandas.read_csv(\"../data/customer-segmentation/train.csv\")\n",
    "\n",
    "# 目的変数を抽出\n",
    "y = train_df.pop('Segmentation').values\n",
    "# 本当はあまり良くないがNanを全て-1で埋める\n",
    "train_df = label_encoding(train_df).fillna(-1)\n",
    "# 顧客のユニークなIDを含むとノイズになるのでカラムを削除する\n",
    "train_df = train_df.drop(\"ID\", axis=1)\n",
    "\n",
    "# 学習データと検証用データに分ける\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# モデルは決定木を利用する\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 検証用データで予測\n",
    "y_val_hat = clf.predict(X_val)\n",
    "\n",
    "# averageという引数に'weighted'という文字列を渡すと\n",
    "# Weighted f1_scoreを計算する\n",
    "print(\"customer-segmentationデータセットでのWeighted F1スコア:\", round(f1_score(y_val, y_val_hat, average='weighted'), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa95e40",
   "metadata": {},
   "source": [
    "## 評価指標を求める上で便利な関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beea8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 学習用データの読み込み\n",
    "train_df = pandas.read_csv(\"../data/customer-segmentation/train.csv\")\n",
    "\n",
    "# 目的変数を抽出\n",
    "y = train_df.pop('Segmentation').values\n",
    "# 本当はあまり良くないがNanを全て-1で埋める\n",
    "train_df = label_encoding(train_df).fillna(-1)\n",
    "# 顧客のユニークなIDを含むとノイズになるのでカラムを削除する\n",
    "train_df = train_df.drop(\"ID\", axis=1)\n",
    "\n",
    "# 学習データと検証用データに分ける\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# モデルは決定木を利用する\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 検証用データで予測\n",
    "y_val_hat = clf.predict(X_val)\n",
    "\n",
    "# 分類結果の評価レポートを表示する\n",
    "print(classification_report(y_val, y_val_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d7e299",
   "metadata": {},
   "source": [
    "## Micro ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd65eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# ダミーデータを作る\n",
    "X, y = make_classification(n_samples = 10000, random_state=42, n_classes=3, n_clusters_per_class=1, weights = [0.899, 0.1, 0.001])\n",
    "# 学習データと検証用データに分ける\n",
    "X_train, X_test, y_train, y_val_dummy = train_test_split(\n",
    "        X, y, random_state=42)\n",
    "\n",
    "# 学習モデルには確率値を出力しやすいロジスティック回帰を用いる\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# クラスごとに0,1のバイナリにして、roc_curveを求めるために\n",
    "# OneHotEncodingを事前に行う\n",
    "# OneHotEncodingがやっていることは以下の通り\n",
    "# 入力: [0, 1, 2] -> 出力: [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
    "y_val_dummy_hat = clf.predict_proba(X_test)\n",
    "y_val_dummy_one_hot = label_binarize(y_val_dummy, classes=clf.classes_)\n",
    "\n",
    "# 各クラスのroc_curveを算出する\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "# クラスごとのROC曲線の横軸(FPR)と縦軸(TPR)に加え、それらを用いてAUCを算出\n",
    "for i in range(len(clf.classes_)):\n",
    "    # クラスごとのROC曲線の横軸(FPR)と縦軸(TPR)の計算\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_val_dummy_one_hot[:, i], y_val_dummy_hat[:, i])\n",
    "    # クラスごとのROC-AUCを算出\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# ROC曲線の横軸(FPR)と縦軸(TPR)を算出(Micro)\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_val_dummy_one_hot.ravel(), y_val_dummy_hat.ravel())\n",
    "# Micro ROC-AUCを算出\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "plt.figure()\n",
    "# 辞書のキーごとにグラフを描画\n",
    "for index, class_key in enumerate(fpr):\n",
    "    # キーがmicroの時はグラフの凡例にclassをつけない\n",
    "    if class_key == \"micro\":\n",
    "        label = \"{} ROC curve (area = {:.2})\".format(class_key, roc_auc[class_key])\n",
    "    else:\n",
    "        label = \"class {} ROC curve (area = {:.2})\".format(clf.classes_[class_key], roc_auc[class_key])\n",
    "    # ROC曲線を描画する\n",
    "    plt.plot(fpr[class_key], tpr[class_key], label=label)\n",
    "\n",
    "# AUCが0.5の時の直線を点線で描画\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "# x軸の描画範囲を0~1にする\n",
    "plt.xlim([0.0, 1.0])\n",
    "# y軸の描画範囲を0~1にする\n",
    "plt.ylim([0.0, 1.0])\n",
    "# x軸とy軸に名前をつける\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# 凡例をグラフの右下部分に表示する\n",
    "plt.legend(loc=\"lower right\")\n",
    "# グラフを表示する\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db494037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用データの読み込み\n",
    "train_df = pandas.read_csv(\"../data/customer-segmentation/train.csv\")\n",
    "\n",
    "# 目的変数を抽出\n",
    "y = train_df.pop('Segmentation').values\n",
    "# 本当はあまり良くないがNanを全て-1で埋める\n",
    "train_df = label_encoding(train_df).fillna(-1)\n",
    "# 顧客のユニークなIDを含むとノイズになるのでカラムを削除する\n",
    "train_df = train_df.drop(\"ID\", axis=1)\n",
    "\n",
    "# 学習データと検証用データに分ける\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# 学習モデルには確率値を出力しやすいロジスティック回帰を用いる\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# クラスごとに0,1のバイナリにして、roc_curveを求めるために\n",
    "# OneHotEncodingを事前に行う\n",
    "# OneHotEncodingがやっていることは以下の通り\n",
    "# 入力: [\"A\", \"B\"] -> 出力: [[1, 0, 0, 0], [0, 1, 0, 0]]\n",
    "y_val_proba_hat = clf.predict_proba(X_val)\n",
    "y_val_one_hot = label_binarize(y_val, classes=clf.classes_)\n",
    "\n",
    "# 各クラスのroc_curveを算出する\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "# クラスごとのROC曲線の横軸(FPR)と縦軸(TPR)に加え、それらを用いてAUCを算出\n",
    "for i in range(len(clf.classes_)):\n",
    "    # クラスごとのROC曲線の横軸(FPR)と縦軸(TPR)の計算\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_val_one_hot[:, i], y_val_proba_hat[:, i])\n",
    "    # クラスごとのROC-AUCを算出\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Micro ROC-AUCを算出する\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_val_one_hot.ravel(), y_val_proba_hat.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "plt.figure()\n",
    "# 辞書のキーごとにグラフを描画\n",
    "for index, class_key in enumerate(fpr):\n",
    "    # キーがmicroのときはグラフの凡例にclassをつけない\n",
    "    if class_key == \"micro\":\n",
    "        label = \"{} ROC curve (area = {:.2})\".format(class_key, roc_auc[class_key])\n",
    "    else:\n",
    "        label = \"class {} ROC curve (area = {:.2})\".format(clf.classes_[class_key], roc_auc[class_key])\n",
    "    # ROC曲線を描画する\n",
    "    plt.plot(fpr[class_key], tpr[class_key], label=label)\n",
    "\n",
    "# AUCが0.5のときの直線を点線で描画\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "# x軸の描画範囲を0~1にする\n",
    "plt.xlim([0.0, 1.0])\n",
    "# y軸の描画範囲を0~1にする\n",
    "plt.ylim([0.0, 1.0])\n",
    "# x軸とy軸に名前をつける\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# 凡例をグラフの右下部分に表示する\n",
    "plt.legend(loc=\"lower right\")\n",
    "# グラフを表示する\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22115d31",
   "metadata": {},
   "source": [
    "## Macro ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1174b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# ダミーデータ\n",
    "X, y = make_classification(n_samples = 10000, random_state=42, n_classes=3, n_clusters_per_class=1, weights = [0.899, 0.1, 0.001])\n",
    "X_train, X_test, y_train, y_val_dummy = train_test_split(\n",
    "        X, y, random_state=42)\n",
    "\n",
    "# 学習モデルには確率値を出力しやすいロジスティック回帰を用いる\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# クラスごとに0,1のバイナリにして、roc_curveを求めるために\n",
    "# OneHotEncodingを事前に行う\n",
    "# OneHotEncodingがやっていることは以下の通り\n",
    "# 入力: [\"A\", \"B\"] -> 出力: [[1, 0, 0, 0], [0, 1, 0, 0]]\n",
    "y_val_dummy_hat = clf.predict_proba(X_test)\n",
    "y_val_dummy_one_hot = label_binarize(y_val_dummy, classes=clf.classes_)\n",
    "\n",
    "# 各クラスのroc_curveを算出する\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "# クラスごとのROC曲線の横軸(FPR)と縦軸(TPR)に加え、それらを用いてAUCを算出\n",
    "for i in range(len(clf.classes_)):\n",
    "    # クラスごとのROC曲線の横軸(FPR)と縦軸(TPR)の計算\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_val_dummy_one_hot[:, i], y_val_dummy_hat[:, i])\n",
    "    # クラスごとのROC-AUCを算出\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Micro ROC-AUCを算出する\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_val_dummy_one_hot.ravel(), y_val_dummy_hat.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "n_classes = len(clf.classes_)\n",
    "# 各クラスで存在する点のx座標を重複を除いて取得する\n",
    "# 重複を削除している理由は各x座標での平均値を取る計算をする際に、\n",
    "# 重複しているものがあった場合同じx座標で無駄に平均値を算出してしまうから\n",
    "# 例えば、重複があるとこれで集まったx座標が[0.5, 0.5, 0.7]だったとき、\n",
    "# 0.5の平均値を2度算出することになってしまう。\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# 平均tprを入れるarrayを定義\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    # 各クラスでall_fprに存在している線形補完して、\n",
    "    # x座標と同じx座標の時のy座標を全て足し合わせる。\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# クラス数で割ることでROC曲線の平均値を算出する\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "# FPRにはy座標の算出に利用したx座標を入れる\n",
    "fpr[\"macro\"] = all_fpr\n",
    "# TPRにROC曲線の平均値を入れる\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "# AUCを算出する\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "plt.figure()\n",
    "# Micro ROC曲線を描画\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         linewidth=2)\n",
    "\n",
    "# Macro ROC曲線を描画\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         linewidth=2)\n",
    "\n",
    "# 各クラスのROC曲線を描画\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                                   ''.format(clf.classes_[i], roc_auc[i]))\n",
    "\n",
    "# AUCが0.5の時の直線を点線で描画\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "# x軸の描画範囲を0~1にする\n",
    "plt.xlim([0.0, 1.0])\n",
    "# y軸の描画範囲を0~1にする\n",
    "plt.ylim([0.0, 1.0])\n",
    "# x軸とy軸に名前をつける\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# 凡例をグラフの右下部分に表示する\n",
    "plt.legend(loc=\"lower right\")\n",
    "# グラフを表示する\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0201d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用データの読み込み\n",
    "train_df = pandas.read_csv(\"../data/customer-segmentation/train.csv\")\n",
    "\n",
    "# 目的変数を抽出\n",
    "y = train_df.pop('Segmentation').values\n",
    "# 本当はあまり良くないがNanを全て-1で埋める\n",
    "train_df = label_encoding(train_df).fillna(-1)\n",
    "# 顧客のユニークなIDを含むとノイズになるのでカラムを削除する\n",
    "train_df = train_df.drop(\"ID\", axis=1)\n",
    "\n",
    "# 学習データと検証用データに分ける\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# 学習モデルには確率値を出力しやすいロジスティック回帰を用いる\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# クラスごとに0,1のバイナリにして、roc_curveを求めるために\n",
    "# OneHotEncodingを事前に行う\n",
    "# OneHotEncodingがやっていることは以下の通り\n",
    "# 入力: [\"A\", \"B\"] -> 出力: [[1, 0, 0, 0], [0, 1, 0, 0]]\n",
    "y_val_proba_hat = clf.predict_proba(X_val)\n",
    "y_val_one_hot = label_binarize(y_val, classes=clf.classes_)\n",
    "\n",
    "# 各クラスのroc_curveを算出する\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "# クラスごとのROC曲線の横軸(FPR)と縦軸(TPR)に加え、それらを用いてAUCを算出\n",
    "for i in range(len(clf.classes_)):\n",
    "    # クラスごとのROC曲線の横軸(FPR)と縦軸(TPR)の計算\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_val_one_hot[:, i], y_val_proba_hat[:, i])\n",
    "    # クラスごとのROC-AUCを算出\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Micro ROC-AUCを算出する\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_val_one_hot.ravel(), y_val_proba_hat.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "n_classes = len(clf.classes_)\n",
    "# 各クラスで存在する点のx座標を重複を除いて取得する\n",
    "# 重複を削除している理由は各x座標での平均値を取る計算をする際に、\n",
    "# 重複しているものがあった場合同じx座標で無駄に平均値を算出してしまうから\n",
    "# 例えば、重複があるとこれで集まったx座標が[0.5, 0.5, 0.7]だったとき、\n",
    "# 0.5の平均値を2度算出することになってしまう。\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# 平均tprを入れるarrayを定義\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    # 各クラスでall_fprに存在している線形補完して、\n",
    "    # x座標と同じx座標の時のy座標を全て足し合わせる。\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# クラス数で割ることでROC曲線の平均値を算出する\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "# FPRにはy座標の算出に利用したx座標を入れる\n",
    "fpr[\"macro\"] = all_fpr\n",
    "# TPRにROC曲線の平均値を入れる\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "# AUCを算出する\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "plt.figure()\n",
    "# Micro ROC曲線を描画\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         linewidth=2)\n",
    "\n",
    "# Macro ROC曲線を描画\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         linewidth=2)\n",
    "\n",
    "# 各クラスのROC曲線を描画\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                                   ''.format(clf.classes_[i], roc_auc[i]))\n",
    "\n",
    "# AUCが0.5のときの直線を点線で描画\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "# x軸の描画範囲を0~1にする\n",
    "plt.xlim([0.0, 1.0])\n",
    "# y軸の描画範囲を0~1にする\n",
    "plt.ylim([0.0, 1.0])\n",
    "# x軸とy軸に名前をつける\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# 凡例をグラフの右下部分に表示する\n",
    "plt.legend(loc=\"lower right\")\n",
    "# グラフを表示する\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03165c59",
   "metadata": {},
   "source": [
    "## customer-segmentation データセットで作成したモデルを評価する上で最適な評価指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2eaa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# 学習用データの読み込み\n",
    "train_df = pandas.read_csv(\"../data/customer-segmentation/train.csv\")\n",
    "\n",
    "# 目的変数を抽出\n",
    "y = train_df.pop('Segmentation').values\n",
    "# 本当はあまり良くないがNanを全て-1で埋める\n",
    "train_df = label_encoding(train_df).fillna(-1)\n",
    "# 顧客のユニークなIDを含むとノイズになるのでカラムを削除する\n",
    "train_df = train_df.drop(\"ID\", axis=1)\n",
    "\n",
    "# 学習データと検証用データに分ける\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# モデルは決定木を利用する\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 検証用データで予測\n",
    "y_val_hat = clf.predict(X_val)\n",
    "print(\"precision: \", precision_score(y_val, y_val_hat, average=None))\n",
    "print(\"recall: \", recall_score(y_val, y_val_hat, average=None))\n",
    "print(\"f1: \", f1_score(y_val, y_val_hat, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747544f5",
   "metadata": {},
   "source": [
    "## ビジネスインパクトの期待値を計算する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb8e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import pandas\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 文字列や数値で表されたラベルを0~(ラベル種類数-1)に変換する関数\n",
    "def label_encoding(df):\n",
    "    object_type_column_list = [c for c in df.columns if df[c].dtypes=='object']\n",
    "    for object_type_column in object_type_column_list:\n",
    "        label_encoder = LabelEncoder()\n",
    "        df[object_type_column] = label_encoder.fit_transform(df[object_type_column])\n",
    "    return df\n",
    "\n",
    "# 学習用データの読み込み\n",
    "train_df = pandas.read_csv(\"../data/customer-segmentation/train.csv\")\n",
    "\n",
    "# 目的変数を抽出\n",
    "y = train_df.pop('Segmentation').values\n",
    "# 本当はあまり良くないがNanを全て-1で埋める\n",
    "train_df = label_encoding(train_df).fillna(-1)\n",
    "# 顧客のユニークなIDを含むとノイズになるのでカラムを削除する\n",
    "train_df = train_df.drop(\"ID\", axis=1)\n",
    "\n",
    "# 学習データと検証用データに分ける\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# モデルは決定木を利用する\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 検証用データで予測\n",
    "y_val_hat = clf.predict(X_val)\n",
    "\n",
    "# 混同行列を作成\n",
    "conf_matrix = confusion_matrix(y_val, y_val_hat)\n",
    "\n",
    "# 分類に用いられるクラス名を定義する\n",
    "labels = [\"A\", \"B\", \"C\", \"D\"]\n",
    "# コスト行列を設定\n",
    "cost_matrix = np.array([[1000, 0, 0, 0], [0, 4000, 0, 0], [0, 0, 2000, 0], [0, 0, 0, 0]])\n",
    "# 混同行列と対応するコスト行列の要素の積を求めて各要素ごとの利益を計算し、最後に要素ごとの合計を計算することで利益を計算する\n",
    "benefit = (pandas.DataFrame(conf_matrix, index=labels, columns=labels).values * cost_matrix).sum()\n",
    "print(\"ビジネスインパクトの期待値:\", benefit, \"円\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6238a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# コスト行列を設定\n",
    "cost_matrix = np.array([[1000, 0, 0, 0], [0, 4000, 0, 0], [0, 0, 2000, 0], [0, 0, 0, 0]])\n",
    "# 混同行列の各要素の人数の割合を求める\n",
    "conf_matrix_rate = conf_matrix / conf_matrix.sum()\n",
    "# 混同行列の各要素の人数の割合と対応するコスト行列の要素の積を求めて各要素ごとの利益を計算し、最後に要素ごとの合計を計算することで利益を計算する\n",
    "benefit = (pandas.DataFrame(conf_matrix_rate, index=labels, columns=labels).values * cost_matrix).sum()\n",
    "print(\"1人あたりのビジネスインパクトの期待値:\", round(benefit, 2), \"円\")\n",
    "print(\"10000人に紹介メールを送るときのビジネスインパクトの期待値:\", round(benefit * 10000), \"円\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c4b7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
