{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2be4f1e",
   "metadata": {},
   "source": [
    "# 3章 二値分類における評価指標"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267282be",
   "metadata": {},
   "source": [
    "## 3.3 混同行列 (Confusion Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e309062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 文字列や数値で表されたラベルを，0~(ラベル種類数-1)に変換して特徴を上書きする関数\n",
    "def label_encoding(df):\n",
    "    # object(今回の場合は文字列)のカラムを取得\n",
    "    object_type_column_list = [c for c in df.columns if df[c].dtypes=='object']\n",
    "    for object_type_column in object_type_column_list:\n",
    "        # LabelEncoderを使ってカテゴリで示されるデータ(質的変数)を数値に変換\n",
    "        label_encoder = LabelEncoder()\n",
    "        df[object_type_column] = label_encoder.fit_transform(df[object_type_column])\n",
    "    return df\n",
    "\n",
    "# 特徴を標準化する関数\n",
    "def standard_scale(X_train, X_val):\n",
    "    ss = StandardScaler()\n",
    "    # 学習用データの特徴を標準化する\n",
    "    X_train = ss.fit_transform(X_train)\n",
    "    # 検証用データの特徴を標準化する\n",
    "    X_val = ss.transform(X_val)\n",
    "    return X_train, X_val\n",
    "\n",
    "# 前処理を実行した学習用データを返す関数\n",
    "def run_preprocess(train_df):\n",
    "    # Nanが含まれており、ロジスティック回帰がそのままでは動かないためNanを含むカラムを削除する\n",
    "    # 本来はNanを埋めるなどの処理をした方が分類性能は上がることが多いが、\n",
    "    # 本書では興味の対象ではないため、説明を簡単にするためにカラムごと削除する\n",
    "    remove_column_list = ['previous_year_rating', 'education']\n",
    "    train_df.drop(remove_column_list, axis=1, inplace=True)\n",
    "    train_df = label_encoding(train_df)\n",
    "    return train_df\n",
    "\n",
    "# train.csvデータをpandasで読み込む\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "\n",
    "# 前処理する\n",
    "train_df = run_preprocess(train_df)\n",
    "# 教師ラベルを取り出す\n",
    "y = train_df.pop('is_promoted').values\n",
    "\n",
    "# 学習用データと検証用データを分離する\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df, y, test_size=0.33, random_state=42)\n",
    "# 学習用データと検証用データを標準化する\n",
    "X_train, X_val = standard_scale(X_train, X_val)\n",
    "\n",
    "# ロジスティック回帰モデルを学習する\n",
    "clf = LogisticRegression(max_iter=200, random_state=2020)\n",
    "clf.fit(X_train, y_train)\n",
    "# ロジスティック回帰モデルで推論を行う\n",
    "y_val_hat = clf.predict(X_val)\n",
    "\n",
    "# 混同行列を作成する\n",
    "conf_matrix = confusion_matrix(y_val, y_val_hat, labels=[1, 0])\n",
    "df_confusion_matrix = pd.DataFrame(conf_matrix, columns=[\"Positive\", \"Negative\"], index=[\"Positive\", \"Negative\"])\n",
    "\n",
    "# 作成した混同行列をヒートマップの形で描画する\n",
    "sns.heatmap(df_confusion_matrix, fmt=\"d\", annot=True, cmap=\"Blues\", annot_kws={\"fontsize\": 20})\n",
    "plt.xlabel(\"model output class\")\n",
    "plt.ylabel(\"actual class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30fe222",
   "metadata": {},
   "source": [
    "## 3.4.1 Employee Promotion Data で正解率を算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859547ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 検証用データの正解率を求める\n",
    "accuracy = accuracy_score(y_val, y_val_hat)\n",
    "print(f\"正解率: {round(accuracy, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991461cf",
   "metadata": {},
   "source": [
    "## 3.5.1 Employee Promotion Data でMCCを算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53a188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "# 検証用データのマシューズ相関係数を算出する\n",
    "mcc = matthews_corrcoef(y_val, y_val_hat)\n",
    "print(f\"MCC: {round(mcc, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ad4e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 実際の正解クラスと予測されたクラスを逆にして検証用データのマシューズ相関係数を算出する\n",
    "mcc = matthews_corrcoef(np.where(y_val == 0, 1, 0), np.where(y_val_hat == 0, 1, 0))\n",
    "print(f\"MCC: {round(mcc, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4031222a",
   "metadata": {},
   "source": [
    "## 3.6.1 Employee Promotion Data で適合率を算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4884301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# 検証用データの適合率を算出する\n",
    "precision = precision_score(y_val, y_val_hat)\n",
    "print(f\"適合率: {round(precision, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c659caab",
   "metadata": {},
   "source": [
    "## 3.7.1 Employee Promotion Data で再現率を算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca31de16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# 検証用データの再現率を算出する\n",
    "recall = recall_score(y_val, y_val_hat)\n",
    "print(f\"再現率: {round(recall, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e5a63e",
   "metadata": {},
   "source": [
    "## 3.8.1 Employee Promotion Data でF1スコアを算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572dfbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 検証用データのF1スコアを算出する\n",
    "f1 = f1_score(y_val, y_val_hat)\n",
    "print(f\"F1スコア: {round(f1, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0323a1",
   "metadata": {},
   "source": [
    "## 3.9.1 Employee Promotion Data でG-Meanを算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b4120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# G-Meanを算出するための関数\n",
    "def g_mean_score(y, y_hat):\n",
    "    # 混同行列を作成して各要素を変数に格納する\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_hat).ravel()\n",
    "    # True Positive Rateを算出する\n",
    "    tp_rate =  tp / (tp + fn)\n",
    "    print(\"True Positive Rate:\", round(tp_rate, 2))\n",
    "    # True Negative Rateを算出する\n",
    "    tn_rate = tn / (tn + fp)\n",
    "    print(\"True Negative Rate:\", round(tn_rate, 2))\n",
    "    # G-meanはTrue Positive RateとTrue Negative Rateの幾何平均を計算\n",
    "    return math.sqrt(tp_rate * tn_rate)\n",
    "\n",
    "print(\"G-Mean: \", round(g_mean_score(y_val, y_val_hat), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b27ed9",
   "metadata": {},
   "source": [
    "## 3.10.1 Employee Promotion Data でROC-AUCを算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6270810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ラベルが1である予測確率を取得する\n",
    "y_val_hat = clf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# ROC-AUCを算出する\n",
    "roc_auc = roc_auc_score(y_val, y_val_hat)\n",
    "print(f\"ROC-AUC: {round(roc_auc, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28944af",
   "metadata": {},
   "source": [
    "## 3.11.2 Employee Promotion Data でPR-AUCを算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288c031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, precision_recall_curve\n",
    "\n",
    "# ラベルが1である予測確率を取得する\n",
    "y_val_hat = clf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# 予測確率を使って閾値を操作したPrecisionとRecallとその時の閾値の3つを算出\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, y_val_hat)\n",
    "# AUCを算出する\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "print(f\"PR-AUC: {round(pr_auc, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f177b91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# 実行した時に同じ値になるように乱数のシードを固定する\n",
    "random.seed(2022)\n",
    "# 0から１の間の小数点つきの数値をy_valの長さ分だけ生成する\n",
    "random_predict = [random.random() for _ in range(len(y_val))]\n",
    "\n",
    "# ランダムに生成した値を使って閾値を操作したPrecisionとRecallとその時の閾値の3つを算出\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, random_predict)\n",
    "# AUCを算出する\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "print(f\"PR-AUC(Random Model): {round(pr_auc, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e23e6c2",
   "metadata": {},
   "source": [
    "## 3.12 pAUC (partial AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791bb6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# ラベルが1である予測確率を取得する\n",
    "y_val_hat = clf.predict_proba(X_val)[:, 1]\n",
    "# 閾値を操作したときのFalse Positive Rate と True Positive Rateとその時の閾値の値を算出\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_val_hat)\n",
    "# ROC曲線を描画しつつ、AUCの値も凡例に記載する\n",
    "plt.plot(fpr, tpr, label=\"ROC curve (area = %0.2f)\" % roc_auc_score(y_val, y_val_hat),)\n",
    "\n",
    "# 0~0.2の範囲でのpAUCの計算\n",
    "p_auc = roc_auc_score(y_val, y_val_hat, max_fpr=0.2)\n",
    "# False Positive Rate が0.2以下の範囲でのFalse Positive Rate と True Positive Rateのみ取得する\n",
    "pfpr = fpr[fpr <= 0.2]\n",
    "ptpr = tpr[fpr <= 0.2]\n",
    "# pAUCの対象範囲の線を描画する。\n",
    "plt.plot(pfpr, ptpr, label=\"pAUC curve (area = %0.2f)\" % p_auc,)\n",
    "# pAUCの対象範囲を塗りつぶす\n",
    "plt.fill_between(pfpr, ptpr, 0, facecolor='darkorange', alpha=0.5)\n",
    "# 凡例を表示する\n",
    "plt.legend(loc=\"lower right\")\n",
    "# 横軸の名前をFalse positive rateにする\n",
    "plt.xlabel(\"False positive rate\")\n",
    "# 縦軸の名前をTrue positive rateにする\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "# pAUCの塗りつぶし部分に\"pAUC\"というテキストを表示する\n",
    "plt.text(0.05, 0.15, \"pAUC\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e269f5",
   "metadata": {},
   "source": [
    "## 3.12.1 Employee Promotion Data でpAUCを算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02e970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ラベルが1である予測確率を取得する\n",
    "y_val_hat = clf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# pAUCを算出\n",
    "p_auc = roc_auc_score(y_val, y_val_hat, max_fpr=0.2)\n",
    "print(f\"pAUC: {round(p_auc, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8af1401",
   "metadata": {},
   "source": [
    "## 3.13 クラスの分布の変化による評価指標への影響"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df243891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc, precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 検証用データのクラスの分布の比率を操作するための関数(weightsは1つ目が Positive の比率, 2つ目が Negative の比率を表す)\n",
    "def calc_custom_storategy(y_val, weights=(1, 1)):\n",
    "    # NegativeとPositiveの数を算出する\n",
    "    is_promoted_count = np.bincount(y_val)\n",
    "    strategy = {0: weights[0] * is_promoted_count[1], 1: weights[1] * is_promoted_count[1]}\n",
    "    return strategy\n",
    "\n",
    "# ここまでで紹介した全ての評価指標で評価を行う関数\n",
    "def evaluate(X_resampled, y_resampled):\n",
    "    y_val_hat = clf.predict(X_resampled)\n",
    "    accuracy = accuracy_score(y_resampled, y_val_hat)\n",
    "    print(f\"accuracy: {round(accuracy, 2)}\")\n",
    "\n",
    "    precision = precision_score(y_resampled, y_val_hat)\n",
    "    print(f\"precision: {round(precision, 2)}\")\n",
    "\n",
    "    recall = recall_score(y_resampled, y_val_hat)\n",
    "    print(f\"recall: {round(recall, 2)}\")\n",
    "\n",
    "    f1score = f1_score(y_resampled, y_val_hat)\n",
    "    print(f\"f1 score: {round(f1score, 2)}\")\n",
    "\n",
    "    mcc = matthews_corrcoef(y_resampled, y_val_hat)\n",
    "    print(f\"MCC: {round(mcc, 2)}\")\n",
    "\n",
    "    g_mean = g_mean_score(y_resampled, y_val_hat)\n",
    "    print(f\"G-Mean: {round(g_mean, 2)}\")\n",
    "\n",
    "    y_val_hat =clf.predict_proba(X_resampled)[:, 1]\n",
    "    roc_auc = roc_auc_score(y_resampled, y_val_hat)\n",
    "    print(f\"ROC-AUC: {round(roc_auc, 2)}\")\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_resampled, y_val_hat)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    print(f\"PR-AUC: {round(pr_auc, 2)}\")\n",
    "\n",
    "    p_auc = roc_auc_score(y_resampled, y_val_hat, max_fpr=0.2)\n",
    "    print(f\"pAUC: {round(p_auc, 2)}\")\n",
    "\n",
    "# 学習データをデータフレームで読み込む\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "\n",
    "# 前処理を行う\n",
    "train_df = run_preprocess(train_df)\n",
    "# 教師ラベルを取り出す\n",
    "y = train_df.pop('is_promoted').values\n",
    "\n",
    "# 学習用データと検証用データを分離する\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df, y, test_size=0.33, random_state=42)\n",
    "X_train, X_val = standard_scale(X_train, X_val)\n",
    "\n",
    "# ロジスティック回帰モデルを学習する\n",
    "clf = LogisticRegression(max_iter=200, random_state=2020)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 評価用のデータのクラスの分布をPositive: Negative = 1:1にする\n",
    "strategy = calc_custom_storategy(y_val, weights=(1, 1))\n",
    "rus = RandomUnderSampler(random_state=0, sampling_strategy=strategy)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_val, y_val)\n",
    "\n",
    "# ここまでで紹介した全ての評価指標で評価する\n",
    "evaluate(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6b46ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価用のデータのクラスの分布をPositive: Negative = 10:1にする\n",
    "strategy = calc_custom_storategy(y_val, weights=(10, 1))\n",
    "rus = RandomUnderSampler(random_state=0, sampling_strategy=strategy)\n",
    "X_resampled2, y_resampled2 = rus.fit_resample(X_val, y_val)\n",
    "\n",
    "evaluate(X_resampled2, y_resampled2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff3cde",
   "metadata": {},
   "source": [
    "## 3.16.4 ビジネスインパクトによる閾値調整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7acdcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# 期待値を算出するための関数\n",
    "def calc_expected_value(conf_matrix):\n",
    "    # コスト行列\n",
    "    TP_COST, FP_COST, TN_COST, FN_COST = 9500, -500, 0, 0\n",
    "    tp, fn, fp, tn = conf_matrix.flatten()\n",
    "\n",
    "    # 各項目の出現確率を算出\n",
    "    number_of_sample = np.sum(conf_matrix)\n",
    "    tp_rate = tp / number_of_sample\n",
    "    fp_rate = fp / number_of_sample\n",
    "    tn_rate = tn / number_of_sample\n",
    "    fn_rate = fn / number_of_sample\n",
    "    # 利益の期待値を算出\n",
    "    return (tp_rate * TP_COST) + (fp_rate * FP_COST) + (tn_rate * TN_COST) + (fn_rate * FN_COST)\n",
    "\n",
    "# 利益曲線を描画する関数\n",
    "def benefit_curve(clf_list, X_val, y_val):\n",
    "    # 分類器ごとに利益曲線を描画する\n",
    "    for clf in clf_list:\n",
    "        y_val_hat_proba = clf.predict_proba(X_val)\n",
    "\n",
    "        expected_value_list = []\n",
    "        thresh_list = []\n",
    "        for proba in np.sort(y_val_hat_proba[:, 1]):\n",
    "            # 予測したラベル\n",
    "            y_val_hat = (y_val_hat_proba[:, 1] > proba).astype(int)\n",
    "            # 混同行列を作成する\n",
    "            conf_matrix = confusion_matrix(y_val, y_val_hat, labels=[1, 0])\n",
    "            # 期待値を算出\n",
    "            expected_value = calc_expected_value(conf_matrix)\n",
    "            expected_value_list.append(expected_value)\n",
    "            thresh_list.append(proba)\n",
    "        # コストが最大の時のインデックスの番号を取得する\n",
    "        max_cost_index = expected_value_list.index(max(expected_value_list))\n",
    "        plt.plot(thresh_list, expected_value_list, label=f\"{clf.__class__.__name__}\")\n",
    "        print(f\"{clf.__class__.__name__}を使ったときに利益が最大になる閾値: {thresh_list[max_cost_index]}\")\n",
    "    plt.xlabel(\"threshold\")\n",
    "    plt.ylabel(\"expected value\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1), loc='upper right', borderaxespad=0, fontsize=10)\n",
    "    plt.show()\n",
    "\n",
    "train_df = pd.read_csv('../data/banking_dataset/train.csv', sep=\";\")\n",
    "\n",
    "# 前処理する\n",
    "train_df = label_encoding(train_df)\n",
    "# 教師ラベルを取り出す\n",
    "y = train_df.pop('y').values\n",
    "\n",
    "# 学習用データと検証用データを分離する\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df, y, test_size=0.33, random_state=42)\n",
    "X_train, X_val = standard_scale(X_train, X_val)\n",
    "\n",
    "# 学習済みの分類器を clf_list に配置する\n",
    "clf_list = []\n",
    "for clf in [LogisticRegression(max_iter=200, random_state=2020), GaussianNB()]:\n",
    "    clf.fit(X_train, y_train)\n",
    "    clf_list.append(clf)\n",
    "\n",
    "# 利益曲線を描画する\n",
    "benefit_curve(clf_list, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c6864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 閾値を指定する\n",
    "y_val_hat = (clf.predict_proba(X_val)[:, 1] > 0.06102603455402777).astype(int)\n",
    "\n",
    "# 混同行列を作成する\n",
    "conf_matrix = confusion_matrix(y_val, y_val_hat, labels=[1, 0])\n",
    "df_confusion_matrix = pd.DataFrame(conf_matrix, columns=[\"Positive\", \"Negative\"], index=[\"Positive\", \"Negative\"])\n",
    "\n",
    "# 作成した混同行列をヒートマップの形で描画する\n",
    "sns.heatmap(df_confusion_matrix, fmt=\"d\", annot=True, cmap=\"Blues\", annot_kws={\"fontsize\": 20})\n",
    "plt.xlabel(\"model output class\")\n",
    "plt.ylabel(\"actual class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3565eda6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
